# Training Knowledge Pipeline

This pipeline converts materials under `backend/training/knowledge` into a structured `patterns.json` dataset that can be embedded into the knowledge base and referenced by the detector and agent.

## Layout

- `backend/training/knowledge/` – source PDFs/images/GIFs
- `backend/training/processed/` – extracted normalized text (`*.txt`), generated by the extractor
- `backend/training/config/` – `pattern_index.yaml` markers/metadata
- `backend/training/schema/` – `pattern_schema.json` JSON Schema
- `backend/training/patterns.json` – final dataset

## 1) Extract Text

```bash
python backend/training/extract_text.py --knowledge-dir "backend/training/knowledge" --processed-dir "backend/training/processed"
```

Options:
- `--force` re-generates existing outputs
- `--max-pages N` limits pages per PDF for faster iteration

## 2) Index Patterns

Copy the template and fill entries with start/end markers and page hints.

```bash
cp backend/training/config/pattern_index_template.yaml backend/training/config/pattern_index.yaml
# Edit pattern_index.yaml
```

Each pattern entry defines its `pattern_id`, `category`, aliases, and one or more source docs with markers.

## 3) Convert Docs to JSON (Optional)

Generate lightweight JSON files for each processed document. These can be used for ingestion into vector stores or manual labeling.

```bash
python backend/training/convert_docs.py --root backend/training --max-chars 1200
```

- Outputs land in `backend/training/json_docs/`
- `--max-chars` controls chunking; set to `0` to keep individual paragraphs

### CSV exports

If you have CSV representations (for example, text exported from a PDF/OCR pass) run:

```bash
python backend/training/convert_csv_docs.py --root backend/training --max-chars 1200
```

- Reads `backend/training/knowledge/csv/*.csv`
- Emits JSON alongside the other docs in `backend/training/json_docs/`

## 4) Build Pattern Dataset

```bash
python backend/training/build_patterns.py --root backend/training --allow-placeholders
```

This validates against `schema/pattern_schema.json` and writes `patterns.json`.

## 5) Integrate

- Re-embed knowledge with `backend/embed_enhanced_knowledge.py` (point it at `patterns.json`).
- Wire `patterns.json` into detection metadata and agent responses.

## Notes

- OCR requires Tesseract installed on the system (`pytesseract`).
- Table extraction (e.g., `Trade Journal.xls`) can be added later via `camelot`/`pandas`.
