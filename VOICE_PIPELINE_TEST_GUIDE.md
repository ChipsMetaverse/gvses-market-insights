# Voice Pipeline Testing Guide

## Quick Start - 5 Minute Test

**URL**: http://localhost:5175
**Browser**: Chrome/Edge (best support for Web Audio API)
**Action**: Click the üéôÔ∏è floating button (bottom-right corner)

---

## Complete Pipeline Test (Step-by-Step)

### Prerequisites
- ‚úÖ Backend running on port 8000
- ‚úÖ Frontend running on port 5175
- ‚úÖ Browser DevTools open (F12 ‚Üí Console tab)
- ‚úÖ Headphones/speakers connected (for audio output)
- ‚úÖ Microphone available

---

## Test Stages

### Stage 1: UI Initialization ‚úÖ
**Action**: Open http://localhost:5175

**Expected UI**:
- Main dashboard loads
- Right panel shows "VOICE ASSISTANT"
- Message: "üé§ Click mic to start"
- Floating button: üéôÔ∏è (bottom-right, inactive state)

**Expected Console**:
```
üåê OpenAIRealtimeService initialized
üîç Config.relayServerUrl: undefined
```

**‚úÖ Pass Criteria**: Dashboard loads, voice panel visible, no errors

---

### Stage 2: Connection Initiation ‚úÖ
**Action**: Click the üéôÔ∏è floating button

**Expected UI**:
- Button changes to ‚åõ (connecting state)
- Voice panel shows "Listening..." placeholder

**Expected Console**:
```javascript
üéØ handleConnectToggle called, voiceProvider: agent, isConnected: false
üöÄ Connecting to OpenAI Realtime...
üìû About to call startConversation()...
üéôÔ∏è [AGENT VOICE] Step 1: Requesting microphone access...
üé§ [AUDIO PROCESSOR] ========== startRecording() CALLED ==========
üé§ [AUDIO PROCESSOR] Checking navigator.mediaDevices availability: true
üé§ [AUDIO PROCESSOR] Checking getUserMedia availability: true
üé§ [AUDIO PROCESSOR] About to call getUserMedia()...
```

**‚úÖ Pass Criteria**: Console shows microphone request starting

---

### Stage 3: Microphone Permission ‚ö†Ô∏è CRITICAL
**Action**: Browser shows permission dialog ‚Üí Click "Allow"

**Expected Browser Prompt**:
```
[Website] wants to use your microphone
[ Block ] [ Allow ]
```

**Expected Console (after Allow)**:
```javascript
‚úÖ [AUDIO PROCESSOR] getUserMedia() completed successfully!
‚úÖ [AUDIO PROCESSOR] Stream obtained: [MediaStream object]
‚úÖ [AUDIO PROCESSOR] Microphone access granted
üîä AudioContext created, state: running
‚úÖ [AUDIO PROCESSOR] Audio processing started
‚úÖ [AGENT VOICE] Microphone access granted and recording started
```

**‚ùå Failure Modes**:
- **No prompt appears**: Check browser permissions settings
- **"Permission denied"**: User blocked mic, need to reset in browser settings
- **"getUserMedia not supported"**: Browser too old or not HTTPS

**‚úÖ Pass Criteria**: Microphone permission granted, AudioContext running

---

### Stage 4: WebSocket Connection ‚ö†Ô∏è CRITICAL
**Action**: Automatic (happens after mic granted)

**Expected Console**:
```javascript
üåê [AGENT VOICE] Step 2: Connecting to OpenAI Realtime...
üåê OpenAIRealtimeService initialized
üåê Fetching OpenAI relay URL from: http://localhost:8000/elevenlabs/signed-url
üì° Relay URL obtained: wss://api.openai.com/v1/realtime?model=...
üì° Creating RealtimeClient with URL: wss://...
üîå Connecting to OpenAI Realtime API...
‚úÖ RealtimeClient.connect() completed - waiting for session.created
üì° RealtimeEvent: server session.created {...}
üöÄ OpenAI session created - connection established!
‚öôÔ∏è OpenAI session updated: {model: 'gpt-realtime-2025-08-28', ...}
‚úÖ [AGENT VOICE] Connected to OpenAI Realtime
‚úÖ startConversation() completed
```

**Backend Logs** (`/tmp/backend_official_schema.log`):
```
INFO:services.openai_relay_server:WebSocket connection established for session session_xxx
INFO:services.openai_relay_server:‚úÖ Official schema configured: server_vad with manual response control
```

**Expected UI**:
- Button changes to üé§ (active/connected state)
- Voice panel: "Connected" or "Listening..."
- VoiceCommandHelper appears (optional floating panel)

**‚ùå Failure Modes**:
- **"Failed to fetch relay URL"**: Backend not running or CORS issue
- **WebSocket error**: Network issue or invalid API key
- **Session creation timeout**: OpenAI API issue

**‚úÖ Pass Criteria**: Session created, UI shows connected state

---

### Stage 5: Audio Streaming üì§
**Action**: Start speaking (e.g., "What is the price of Tesla?")

**Expected Console (continuous while speaking)**:
```javascript
üé§ [VAD] User started speaking
üì§ Sending 4096 audio samples to OpenAI Realtime API
üì§ Sending 4096 audio samples to OpenAI Realtime API
üì§ Sending 4096 audio samples to OpenAI Realtime API
... (repeats every ~170ms)
```

**Expected UI**:
- Voice panel: "Listening..." message
- Audio level bars animate (if present)

**‚ùå Failure Modes**:
- **No audio samples sent**: Microphone not capturing
- **"AudioContext suspended"**: Click page to activate audio

**‚úÖ Pass Criteria**: Continuous audio samples sent while speaking

---

### Stage 6: Voice Activity Detection (VAD) üõë
**Action**: Stop speaking (pause for 500ms)

**Expected Console**:
```javascript
üõë [VAD] User stopped speaking - manually committing buffer for transcription
```

**What Happens**:
1. OpenAI's server_vad detects silence (> 500ms)
2. Fires `input_audio_buffer.speech_stopped` event
3. Frontend calls `client.commitInputAudio()`
4. Triggers transcription (STT)

**‚ùå Failure Modes**:
- **VAD doesn't trigger**: Threshold too high or background noise
- **Triggers too early**: Threshold too low (0.6 is balanced)

**‚úÖ Pass Criteria**: VAD event fires after speech ends

---

### Stage 7: Speech-to-Text (STT) üìù
**Action**: Automatic (after VAD commit)

**Expected Console**:
```javascript
üìù [USER STT] New user speech started - ID: msg_abc123
üìù [USER STT] Transcript: "what is the price of Tesla"
üìù [USER STT] Transcript: "what is the price of Tesla?" (delta updates)
‚úÖ [COMPLETED] Item completed - Type: message, Role: user, Status: completed
üìù ‚úÖ [FINAL] User speech completed, emitting final transcript: what is the price of Tesla
```

**Expected UI**:
- User message appears in voice panel
- Message bubble: "üë§ what is the price of Tesla?"
- Timestamp shown

**‚ùå Failure Modes**:
- **No transcript**: Transcription model issue or audio quality too poor
- **Partial transcript**: Audio buffer too short
- **Wrong transcript**: Background noise or unclear speech

**‚úÖ Pass Criteria**: Accurate transcript generated and displayed

---

### Stage 8: Agent Processing ü§ñ
**Action**: Automatic (after final transcript)

**Expected Console**:
```javascript
üé§ [AGENT VOICE] Transcript received - Final: true, Text: "what is the price of Tesla"
‚úÖ [AGENT VOICE] User final transcript received, sending to agent: what is the price of Tesla
‚úÖ [AGENT VOICE] Sending to agent: what is the price of Tesla

[Agent Orchestrator]
üìä [AGENT] Calling tool: get_stock_price
  ‚Üí symbol: TSLA
‚úÖ [AGENT] Tool result: {"symbol": "TSLA", "price": 429.86, "change": -13.68, ...}
ü§ñ [AGENT] Generated response: Tesla (TSLA) is currently trading at $429.86, down $13.68 (-3.08%) today...
```

**Backend Logs**:
```
INFO:services.market_service_factory:Using Alpaca for TSLA quote
INFO:services.market_service:Fetching Alpaca quote for TSLA
INFO:services.market_service:Alpaca quote for TSLA: $429.86, open: $443.545
```

**Expected UI**:
- Assistant thinking indicator (optional)
- Chart may update if chart commands present

**‚ùå Failure Modes**:
- **"I couldn't generate a response"**: Agent error or timeout
- **Tool call fails**: Backend API issue (Alpaca/Yahoo)
- **Empty response**: max_tokens too low

**‚úÖ Pass Criteria**: Agent calls tools, generates response with market data

---

### Stage 9: Text-to-Speech (TTS) üì¢
**Action**: Automatic (after agent response)

**Expected Console**:
```javascript
üì§ Sending agent text for TTS: Tesla (TSLA) is currently trading at $429.86...
üõë Cancelled active response: resp_xyz (if any autonomous response)
‚úÖ TTS request sent (autonomous responses cancelled)
```

**OpenAI Response**:
```javascript
üîä Audio delta received: 4800 samples
üîä Audio delta received: 4800 samples
üîä Audio delta received: 4800 samples
... (audio chunks arrive)
```

**‚ùå Failure Modes**:
- **No audio deltas**: TTS not triggered or OpenAI error
- **"response.create failed"**: Session configuration issue

**‚úÖ Pass Criteria**: Audio deltas received from OpenAI

---

### Stage 10: Audio Playback üîä
**Action**: Automatic (audio plays through speakers)

**Expected Console**:
```javascript
üîä playNextAudio: Starting playback
üîä playNextAudio: queueLength: N
[Audio plays...]
üîä Audio playback completed
```

**Expected Experience**:
- Hear agent voice speaking the response
- Voice sounds natural (alloy voice)
- Audio quality clear

**Expected UI**:
- Assistant message appears in voice panel
- Message bubble: "ü§ñ Tesla (TSLA) is currently trading at..."
- Timestamp shown

**‚ùå Failure Modes**:
- **No audio playback**: AudioContext suspended or speakers muted
- **"playNextAudio: Skipping - queueLength: 0"**: Audio queue empty
- **Choppy audio**: Buffer underrun or CPU overload

**‚úÖ Pass Criteria**: Clear voice response plays, message displayed

---

## Success Checklist

After completing all 10 stages, verify:

- [ ] Microphone permission granted
- [ ] WebSocket connected ("session.created" received)
- [ ] Audio samples streaming while speaking
- [ ] VAD detects speech end
- [ ] Accurate transcript generated
- [ ] Agent processes query with tools
- [ ] Market data returned (price, change, etc.)
- [ ] TTS audio chunks received
- [ ] Voice response plays clearly
- [ ] UI shows complete conversation thread

**Full Pipeline Success**: All 10 stages pass ‚úÖ

---

## Quick Debug Commands

### Check Connection Status
```javascript
// In browser console
console.log("Backend health:", await fetch('http://localhost:8000/health').then(r => r.json()));
console.log("Relay URL:", await fetch('http://localhost:8000/elevenlabs/signed-url').then(r => r.json()));
```

### Check Audio Status
```javascript
// AudioContext state
console.log("AudioContext:", window.audioContext?.state); // Should be "running"

// Microphone devices
navigator.mediaDevices.enumerateDevices().then(devices => {
  console.log("Mics:", devices.filter(d => d.kind === 'audioinput'));
});
```

### Force Trigger Events
```javascript
// Manually click voice button
document.querySelector('[data-testid="voice-fab"]')?.click();

// Check current provider
console.log("Voice provider:", document.querySelector('.voice-panel-right')?.textContent);
```

---

## Common Failure Patterns

### Pattern 1: "Loading analysis..." Stuck
**Symptom**: UI frozen on loading state
**Root Cause**: Backend connection failure
**Fix**: Check backend health endpoint, restart backend

### Pattern 2: Mic Granted But No Audio Sent
**Symptom**: Permission granted, but no "üì§ Sending audio samples"
**Root Cause**: AudioContext not created or suspended
**Fix**: Click page to activate audio context

### Pattern 3: Transcript Generated But No Agent Response
**Symptom**: User message appears, but no assistant reply
**Root Cause**: Agent orchestrator connection failure
**Fix**: Check `/ask` endpoint in Network tab

### Pattern 4: Agent Responds But No Voice
**Symptom**: Text response appears, but no audio playback
**Root Cause**: TTS not triggered or audio queue issue
**Fix**: Check console for "üì§ Sending agent text for TTS"

---

## Performance Benchmarks

**Expected Timings** (from mic to voice response):

1. Microphone permission: < 1s (user action)
2. WebSocket connection: 500-1000ms
3. Audio streaming: Real-time (continuous)
4. VAD detection: 500ms (silence threshold)
5. STT transcription: 500-1500ms
6. Agent processing: 1-3s (includes tool calls)
7. TTS generation: 500-1500ms
8. Audio playback: Real-time (streaming)

**Total latency**: 3-7 seconds (speak ‚Üí hear response)

---

## Advanced Diagnostics

### Enable Verbose Logging
Open `useAgentVoiceConversation.ts` and `OpenAIRealtimeService.ts`
All console.log statements already present - no changes needed

### Monitor Network Traffic
1. Open DevTools ‚Üí Network tab
2. Filter: "WS" (WebSocket) + "Fetch/XHR"
3. Look for:
   - `/elevenlabs/signed-url` - Should return 200 OK with wss:// URL
   - WebSocket connection to `api.openai.com` - Should show "101 Switching Protocols"
   - `/ask` - Should POST user query, return 200 OK with agent response

### Backend Logs
```bash
# Watch backend logs in real-time
tail -f /tmp/backend_official_schema.log

# Look for:
# - "‚úÖ Official schema configured"
# - "WebSocket connection established"
# - Session update events
# - Tool execution logs
```

### Test Individual Components

**Test Microphone Only**:
```javascript
navigator.mediaDevices.getUserMedia({ audio: true })
  .then(stream => console.log("‚úÖ Mic works:", stream))
  .catch(err => console.error("‚ùå Mic failed:", err));
```

**Test Backend Health**:
```bash
curl -s http://localhost:8000/health | jq '.openai_relay_ready'
# Should output: true
```

**Test Relay URL**:
```bash
curl -s http://localhost:8000/elevenlabs/signed-url | jq '.url'
# Should output WebSocket URL starting with wss://
```

---

## Next Steps After Testing

### If All Tests Pass ‚úÖ
1. Document any configuration steps needed
2. Create user guide for voice features
3. Consider adding error recovery UI
4. Deploy to production

### If Tests Fail ‚ùå
1. Identify failure stage (1-10 above)
2. Capture console output + error messages
3. Check corresponding code file for that stage
4. Apply targeted fix
5. Re-test from failed stage

---

**Testing Status**: Ready for manual browser verification
**Expected Time**: 5-10 minutes per test cycle
**Tools Needed**: Browser DevTools, speakers, microphone
